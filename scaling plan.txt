scaling plan

scalling the services

search and booking service would scale horizontal by adding loadbalancer infront of those services.
we would also have discovery capabilities. to scale core we would have make sure its highly available by having replica it could easily switch with incase the main went down and we would allow for it to self heal by immediately bring up another replicate. This could be accomplished with our own orchestration logic or using Kubernetes(recommended). Another route we could take is to split out the CRUD operations into its own micro service and make core solely api gateway

Scaling the datastores

For postgres this would be achieved by having primary with read replicas. For event creation we dont need it to be available immediately so eventual consistency is fine here. This does present an issue when it comes to ticket statues more specifically when tickets are sold. To solve this we could introduce another redis cache for sold tickets and enrich any reads of ticket(s) with data from this catch. Some additional computational overhead is introduced on reads but its worth for the improvements to ticket data consistency which is a high priority in a system like this.

Redis: scaling redis is simple just and horizontal.

same for elastic search. another thing to note is in production env at scale i would introduce CDC(change data capture) from postgres -> to -> elasticsearch



Improvements:

quite few improvements could be made to enrich the user experience:

- Location, Location, Location
   Introducing PostGis or any other geospatial db could be quite beneficial cause would enhance the users search capabilities drastically as well as providing a reasonable avenue to get a list of suggested events which we could display below the search bar or result before the user actually attempts a search. While geohash provides a really nice way to reduce a 2 dimensional point ( lat +long) its not perfect so we would need to also setup quadtree that would be initialized in memory from the data in the db when the search service starts. Our rule for division could just be something as simple as no more than 5 future events within on section. Ofcourse we would also need update the quadtree when new events are added or removed. Multiple instances of the search service adds even more complexity. We can just use Redis(geospatial) to deal with this issue.

- Scalping detection and protection. it would be nice to implement some safe guards against bad actors and abuse such as .........

- High traffic events
	we should handle events like this differently due to its their potential to cause massive spikes and overwhelm the system. even if we scale and we have ways to self heal we still run the risk of providing a bad user experience or worse if we naively handle these events in the same way we do other events. simplest and easiest way to handle this in my opening would be to have this evens marked as high traffic events at the time of creation and introduce a virtual line or queue that releases user some set number of user over certain period of time maybe in conjuction with when we release tickets. we could have it so the number of users we take of the queue and direct to reserve/purchase tickets is dynamic. It could be dependent on number of factors such as an evaluation of systems load and availability, number ticket available , .....

- Seat Map
	It would be nice to have more data associated with the venue to allow us to show 
a more true to life layout of the seating arrangement. How ever this would require setting up some json per venue we could parse through and having clean associate between ticket and seat.

- Healthz 
i have some healthz endpoints stubbed out but did not implement them. This is obvious improvement that should be made especially if we would want to achieve some amount of self healing through Kubernetes.

- Logging
 Prometheus with Graphana.

- Global Utils and Types
  All the services are written in go and there quite a bit of code that overlaps when it comes to types and util functions. It would make sense to put it in that code in a utils package at the root level that has it its own go mod so it can be imported into all the services. I can see arguments for and against but given where this project is in development i think this would be fine as well as very beneficial. 

Tradeoff discussion

Atomically operations versus Multi single attempts(need better name/phrase for this)

While it may be a better experience for the user to be able reserve atleast some of the tickets they selected if not all [explain way this route is better]


 

